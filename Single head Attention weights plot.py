# -*- coding: utf-8 -*-
"""normal_adl

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VzonPhvPKVwye8PfNSJY5eUpM10ei-Vq
"""

from tensorflow.keras.layers import GRU
from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import Masking

from collections import Counter
import numpy as np
import pickle

import tensorflow as tf
import tensorflow_datasets
ds = tensorflow_datasets.load('squad/v1.1')
dataset = tensorflow_datasets.as_numpy(ds)

import tensorflow as tf
import tensorflow_datasets
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English
from collections import Counter
import numpy as np
nlp = English()
tokenizer = nlp.Defaults.create_tokenizer(nlp)
def get_tokens(txt):
    return [r.text for r in tokenizer(txt)]

def get_char(token):
    ans = []
    for i in token:
        ans.append(list(i))
    return ans
from tensorflow.keras.preprocessing.sequence import pad_sequences

def equalize_length(txt, m_len):
    return pad_sequences(txt, maxlen = m_len, padding = 'post')

con_w, con_c, que_w, que_c,y1, y2 = [], [], [], [], [], []
break_flag = 0
answer_ary = []
cw, ch, ce, cy = 0,0,0,0
for i in dataset['train']:
    #print('yes')
    if break_flag == 6:
        break
        
    
    context = i['context'].decode('utf-8')
    query = i['question'].decode('utf-8')

    c_token = get_tokens(context)

    c_tmp = get_char(c_token)
    
  
    q_token = get_tokens(query)
    

    q_tmp = get_char(q_token)

    if q_token[0] == 'What' and cw < 3:
      con_w.append(c_token)
      con_c.append(c_tmp)
      que_w.append(q_token)
      que_c.append(q_tmp)
      cw =  cw + 1
    
    if q_token[0] == 'Why' and cy < 3:
      con_w.append(c_token)
      con_c.append(c_tmp)
      que_w.append(q_token)
      que_c.append(q_tmp)
      cy =  cy + 1

    if q_token[0] == 'When' and ce < 3:
      con_w.append(c_token)
      con_c.append(c_tmp)
      que_w.append(q_token)
      que_c.append(q_tmp)
      ce =  ce + 1
    
    if q_token[0] == 'Where' and ch < 3:
      con_w.append(c_token)
      con_c.append(c_tmp)
      que_w.append(q_token)
      que_c.append(q_tmp)
      ch =  ch + 1

    if cw > 2 and ch > 2 and ce > 2 and cy > 2:
      break
    
    
    start_pos = len(get_tokens(context[:i['answers']['answer_start'][0]]))
    end_pos = start_pos + len(get_tokens(i['answers']['text'][0].decode('utf-8'))) - 1
    
    y1.append(start_pos)
    y2.append(end_pos)
    #break_flag = break_flag + 1
    answer_ary.append(get_tokens(i['answers']['text'][0].decode('utf-8')))


answer_ary = [[i for i in sen if i != ' ']for sen in answer_ary]


token_context = [[i for i in sen if i != ' '] for sen in con_w]
#token_context = equalize_length(token_context, 809)
#print(np.shape(token_context[4]))

import pickle
path = 'word_tokenizer.pkl'
with open(path, 'rb') as handle:
    word_tokenizer  = pickle.load(handle)
vocab_size = len(word_tokenizer)

import pickle
path = 'char_tokenizer.pkl'
with open(path, 'rb') as handle:
    char_tokenizer  = pickle.load(handle)

query_ch_token = [[[char_tokenizer[i] if i in char_tokenizer else char_tokenizer['OOV'] for i in words] for words in sen] for sen in que_c]
context_ch_token = [[[char_tokenizer[i] if i in char_tokenizer else char_tokenizer['OOV'] for i in words] for words in sen] for sen in con_c]
query_w_token = [[word_tokenizer[i] if i in word_tokenizer else word_tokenizer['OOV'] for i in sen] for sen in que_w]
context_w_token = [[word_tokenizer[i] if i in word_tokenizer else word_tokenizer['OOV'] for i in sen] for sen in con_w]

from tensorflow.keras.preprocessing.sequence import pad_sequences
max_word_que, max_word_con, max_ch_con, max_ch_que = 60,809,37,26
def equalize_length(txt, m_len):
    return pad_sequences(txt, maxlen = m_len, padding = 'post')
query_ch = []
for i in range(len(query_ch_token)):
    query_ch.append(equalize_length(query_ch_token[i], max_ch_que))
    
query_ch = equalize_length(query_ch, max_word_que)

context_ch = []
for i in range(len(context_ch_token)):
    context_ch.append(equalize_length(context_ch_token[i], max_ch_con))
context_ch = equalize_length(context_ch, max_word_con)
query_w_token = equalize_length(query_w_token, max_word_que)
context_w_token = equalize_length(context_w_token, max_word_con)

with open('embedding.pkl', 'rb') as handle:
    embedding_matrix  = pickle.load(handle)

class encoder(tf.keras.Model):

    def __init__(self, embedding_matrix, num_units, vocab_size):
        super(encoder, self).__init__()
        
        self.embedding_matrix = embedding_matrix
        #self.char_size = char_len
        self.vocab_size = vocab_size
        self.ch_gru_con = Bidirectional(GRU(100, return_sequences = True, return_state = False, dropout=0.1))
        self.ch_gru_que = Bidirectional(GRU(100, return_sequences = True, return_state = False, dropout=0.1))
        self.gru_layer_con1 = Bidirectional(GRU(num_units, return_sequences = True, return_state = False, dropout=0.1))
        self.gru_layer_con2 = Bidirectional(GRU(num_units, return_sequences = True, return_state = False, dropout=0.1))
        self.gru_layer_que1 = Bidirectional(GRU(num_units, return_sequences = True, return_state = False, dropout=0.1))
        self.gru_layer_que2 = Bidirectional(GRU(num_units, return_sequences = True, return_state = False, dropout=0.1))
        self.final_grulayer_con = Bidirectional(GRU(num_units, return_sequences = True, return_state = True, dropout=0.1))
        self.final_grulayer_que = Bidirectional(GRU(num_units, return_sequences = True, return_state = True, dropout=0.1))
        #self.embedding_mask = Embedding(self.vocab_size, 300, weights=[self.embedding_matrix], trainable=False, mask_zero=True)
        self.embedding = Embedding(self.vocab_size, 300, weights=[self.embedding_matrix], trainable=False, mask_zero=True)
        #self.con_mask = Masking(mask_value = 0.0, input_shape = (809,))
        #self.que_mask = Masking(mask_value = 0.0, input_shape = (60,))
        '''self.ch_gru_con._could_use_gpu_kernel = False
        self.ch_gru_que._could_use_gpu_kernel = False
        self.gru_layer_con1._could_use_gpu_kernel = False
        self.gru_layer_con2._could_use_gpu_kernel = False
        self.gru_layer_que1._could_use_gpu_kernel = False
        self.gru_layer_que2._could_use_gpu_kernel = False
        self.final_grulayer_con._could_use_gpu_kernel = False
        self.final_grulayer_que._could_use_gpu_kernel = False'''

        #self.query_embedding = Embedding(self.vocab_size, 300, weights=[self.embedding_matrix], trainable=False)
        
    def __call__(self, w_cont, w_query, ch_cont, ch_query):
        
        #context_cvec = Embedding(self.char_size, self.char_size, trainable=True, mask_zero = True)(ch_cont)
        #query_cvec = Embedding(self.char_size, self.char_size, trainable=True, mask_zero = True)(ch_query)

        context_wvec = self.embedding(w_cont)
        query_wvec = self.embedding(w_query)

        #contextm = self.embedding_mask(w_cont)
        #querym = self.embedding_mask(w_query)

        c_mask = tf.cast((w_cont), tf.bool)
        q_mask = tf.cast((w_query), tf.bool)

        #c_mask = tf.cast(self.con_mask(w_cont), tf.bool)
        #q_mask = tf.cast(self.que_mask(w_query), tf.bool)   
        #c_mask = self.con_mask(w_cont)
        #q_mask = self.que_mask(w_query)

        context_cvec = self.ch_gru_con(ch_cont)
        query_cvec = self.ch_gru_que(ch_query)
        
        context_embed = Concatenate(axis = 2)([context_wvec, context_cvec])
        query_embed = Concatenate(axis = 2)([query_wvec, query_cvec])

        con_en1 = self.gru_layer_con1(context_embed)
        con_en2 = self.gru_layer_con2(con_en1)
        con_en3, c_state1, c_state2 = self.final_grulayer_con(con_en2)

        que_en1 = self.gru_layer_que1(query_embed)
        que_en2 = self.gru_layer_que2(que_en1)
        que_en3, q_state1, q_state2 = self.final_grulayer_que(que_en2)
        
        return con_en3, c_state1, c_state2, que_en3, q_state1, q_state2, c_mask, q_mask
        
    
class GatedAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(GatedAttention, self).__init__()
        self.W1 = Dense(units, use_bias = False)
        self.W2 = Dense(units, use_bias = False)
        self.W3 = Dense(units, use_bias = False)
        self.ans_Dense = Dense(4*units, use_bias = False)
        self.V = Dense(1)

    def __call__(self, query, passage, hidden, mask):
        hidden_passage = tf.expand_dims(hidden, 1)
        passage_dim = tf.expand_dims(passage, 1)
        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(passage_dim) + self.W3(hidden_passage)))
        mask_value = tf.expand_dims((1 - mask) * -1e30, 2)
        mask_score = mask_value + score
        attention_weights = tf.nn.softmax(mask_score, axis=1)
        context_vector = attention_weights * query
        context_vector = tf.reduce_sum(context_vector, axis=1)
        ans = tf.concat([passage, context_vector], axis = 1)
        shape = np.shape(ans)
        ans = self.ans_Dense(ans)
        g = tf.math.sigmoid(ans)

        return g * ans, attention_weights

class GatedAttentionDecoder(tf.keras.Model):
    def __init__(self, dec_units):
        super(GatedAttentionDecoder, self).__init__()
        self.dec_units = dec_units
        self.gru = Bidirectional(GRU(self.dec_units, return_sequences = True, return_state = True))
        self.attention = GatedAttention(self.dec_units)
        #self.gru._could_use_gpu_kernel = False
        
    def __call__(self, query, passage, hidden, mask): # call by passage particular time instant, consider passage as decoder input
        context_vector, weights = self.attention(query, passage, hidden, mask)
        context_vector = tf.expand_dims(context_vector, 1)
        output, state1, state2 = self.gru(context_vector)
        state = tf.concat([state1, state2], axis=-1)
        return output, state, weights


class SelfAttention(tf.keras.Model):
    def __init__(self, units):
        super(SelfAttention, self).__init__()
        self.W1 = Dense(units, use_bias = False)
        self.W2 = Dense(units, use_bias = False)
        self.V = Dense(units)
        
    def __call__(self, passage1, passage2, mask):#passage 2 decoder input
        passage1 = tf.nn.relu(self.W1(passage1))
        passage2 = tf.nn.relu(self.W2(passage2))
        score = tf.matmul(passage1, tf.transpose(passage2, [0, 2, 1]))
        mask_value = tf.expand_dims((1 - mask) * -1e30, 2)
        attention_weights = tf.nn.softmax((score + mask_value), axis=1)
        context_vector = tf.matmul(attention_weights, passage1)
        
        concate = tf.concat([passage2, context_vector], axis = 2)
        
        return concate, attention_weights

class InitialStateoutput(tf.keras.Model):
    def __init__(self, units):
        super(InitialStateoutput, self).__init__()
        self.W1 = Dense(units, use_bias = False)
        self.W2 = Dense(units, use_bias = False)
        self.V = Dense(1)
        
    def __call__(self, query, query_hidden, mask):
        query_hidden = tf.expand_dims(query_hidden, 1)
        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(query_hidden)))
        mask_value = tf.expand_dims((1 - mask) * -1e30, 2)
        mask_score = mask_value + score
        attention_weights = tf.nn.softmax(mask_score, axis=1)
        context_vector = attention_weights * query
        context_vector = tf.reduce_sum(context_vector, axis=1)
    
        return context_vector, attention_weights

class OutputAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(OutputAttention, self).__init__()
        self.W1 = Dense(units, use_bias = False)
        self.W2 = Dense(units, use_bias = False)
        self.V = Dense(1)
        
    def __call__(self, passage, hidden, mask):#passage 2 decoder input
        hidden = tf.expand_dims(hidden, 1)
        score = self.V(tf.nn.tanh(self.W1(passage) + self.W2(hidden)))
        mask_value = tf.expand_dims((1 - mask) * -1e30, 2)
        attention_weights = tf.nn.softmax((score + mask_value), axis=1)
        context_vector = attention_weights * passage
        context_vector = tf.reduce_sum(context_vector, axis=1)
    
        return context_vector, attention_weights

class OutputDecoder(tf.keras.Model):
    def __init__(self, dec_units):
        super(OutputDecoder, self).__init__()
        self.dec_units = dec_units
        self.gru = Bidirectional(GRU(self.dec_units, return_sequences = True, return_state = True))
        self.attention = OutputAttention(self.dec_units)
        #self.gru._could_use_gpu_kernel = False
    
    def __call__(self, passage, hidden, mask): # call by attention hidden state particular time instant, consider passage as decoder input
        context_vector, weights = self.attention(passage, hidden, mask)
        context_vector = tf.expand_dims(context_vector, 1)
        output, state1, state2 = self.gru(context_vector)
        state = tf.concat([state1, state2], axis=-1)
        return output, state, weights

class SAdecode(tf.keras.Model):
    def __init__(self, dec_units):
        super(SAdecode, self).__init__()
        self.dec_units = dec_units
        self.gru = Bidirectional(GRU(self.dec_units, return_sequences = True, return_state = False, dropout=0.05))
        #self.gru._could_use_gpu_kernel = False

    def __call__(self,input):
        output = self.gru(input)
        return output

encoder_lyr = encoder(embedding_matrix, 30, vocab_size)
G_decode = GatedAttentionDecoder(30)
S_attn = SelfAttention(30)
S_decode = SAdecode(30)
O_decode = OutputDecoder(30)
O_initial = InitialStateoutput(30)
optimizer = tf.keras.optimizers.Adadelta(learning_rate = 1, rho = 0.95,epsilon=1e-06)

checkpoint_dir = 'checkpoints/./training_checkpoints/ckpt'
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder_lyr=encoder_lyr,
                                 S_decode=S_decode,
                                 S_attn=S_attn,
                                 G_decode=G_decode,
                                 O_decode = O_decode,
                                 O_initial=O_initial)
checkpoint.restore('checkpoints/./training_checkpoints/ckpt-38')

def attention_plot(context_w_token, query_w_token, context_ch, query_ch):
      enc_c, state_c1, state_c2, enc_q, state_q1, state_q2, c_mask, q_mask = encoder_lyr(context_w_token, query_w_token, context_ch, query_ch)
      shape = np.shape(enc_c)
      #hidden = tf.math.add(state_c1, state_c2)
      #hidden = tf.math.divide(hidden,2)
      state_q = tf.concat([state_q1, state_q2], axis=-1)
      q_mask = tf.cast(enc_q._keras_mask, tf.float32)
      c_mask = tf.cast(enc_c._keras_mask, tf.float32)
      O_hidden, _ = O_initial(enc_q, state_q, q_mask)
      state_q1, state_q2, state_q = [], [], []

      hidden = tf.concat([state_c1, state_c2], axis=-1)
      for i in range(shape[1]):
          output, hidden, attention_wgts = (G_decode(enc_q, enc_c[:, i, :], hidden, q_mask))
          
          if i != 0:
              decode_o = tf.concat([decode_o, output], axis = 1)
              attention = tf.concat([attention, attention_wgts], axis = 2)
          else:
              decode_o = output
              attention = attention_wgts
      enc_c, output, hidden, state_c1, state_c2, enc_q = [], [], [], [], [], []

      SA_o, _ = S_attn(decode_o, decode_o, c_mask)
      attention = attention.numpy()
      return attention

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
def plot_attention(attention, query, context,j):
  fig = plt.figure(figsize=(100,600))
  ax = fig.add_subplot(1, 1, 1)
  ax.matshow(attention, cmap='viridis')

  fontdict = {'fontsize': 30}

  ax.set_yticklabels([' '] + query, fontdict=fontdict, rotation=50)
  ax.set_xticklabels(['  '] + context, fontdict=fontdict, rotation=90)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  fname = 'normal_attn/fig'+str(7000+j)+'.png'
  fig.savefig(fname)



context_ch, query_ch = tf.cast(context_ch, tf.float32), tf.cast(query_ch, tf.float32)
context_w_token, query_w_token = tf.cast(context_w_token, tf.float32), tf.cast(query_w_token, tf.float32)

itr = len(context_w_token)
for j in range(itr):
  w_con, w_que = tf.expand_dims(context_w_token[j], axis = 0), tf.expand_dims(query_w_token[j], axis = 0)
  c_con, c_que = tf.expand_dims(context_ch[j], axis = 0), tf.expand_dims(query_ch[j], axis = 0)
  attention_mat = attention_plot( w_con, w_que, c_con, c_que)
  shape = np.shape(attention_mat)
  attention_mat = tf.reshape(attention_mat, [shape[1], shape[2]])
  shape_q = np.shape(que_w[j])
  shape_c = np.shape(con_w[j])
  attention_mat = attention_mat[:shape_q[0], :shape_c[0]]
  plot_attention(attention_mat, que_w[j], con_w[j],j)