# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IcTtKZ0Yr_nAdNkzQBW0-vIaLea45u2F
"""

!pip install transformers

from transformers import AdamW
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from transformers import RobertaTokenizer, RobertaForQuestionAnswering
import torch

import torchtext
import torch
train_data, validation_data = torchtext.datasets.SQuAD1(root='.data', split=('train', 'dev'))

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForQuestionAnswering.from_pretrained('/content/drive/MyDrive/I3D/Roberta4/')

import numpy as np

class DataModule():
  def __init__(self, tokenizer, train_data, validation_data, batch_size):
    super().__init__()
    self.tokenizer = tokenizer
    self.batch_size = batch_size
    self.train_data = train_data
    self.valid_data = validation_data
    self.train = self.encode_que_pas(self.tokenizer, self.train_data)
    self.valid = self.encode_que_pas(self.tokenizer, self.valid_data)

  def encode_que_pas(self, tokenizer, data, max_length = 500, pad_to_max_length = True, return_tensors = "pt"):
    input_ids = []
    attention_masks = []
    st_pos, en_pos = [], []
    for i in data:
      encode = tokenizer(i[1], i[0], return_tensors='pt', max_length = max_length, padding = 'max_length', truncation = True)
      input_ids.append(encode['input_ids'])
      attention_masks.append(encode['attention_mask'])
      tmp = i[0].split(i[2][0])
      start = len(tokenizer.tokenize(tmp[0]))
      st_pos.append(start)
      en_pos.append(start + len(tokenizer.tokenize(i[2][0])))
    input_ids = torch.cat(input_ids, dim = 0)
    attention_masks = torch.cat(attention_masks, dim = 0)
    st_pos = torch.tensor(st_pos)
    en_pos = torch.tensor(en_pos)

    batch = {
        "input_ids":input_ids,
        "attention_masks":attention_masks,
        "st_pos":st_pos,
        "en_pos":en_pos,
    }
    return batch
  
  def train_loader(self):
    dataset = TensorDataset(self.train['input_ids'], self.train['attention_masks'], self.train['st_pos'], self.train['en_pos'])
    train_load = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)
    return train_load

  def valid_loader(self):
    dataset = TensorDataset(self.train['input_ids'], self.train['attention_masks'], self.train['st_pos'], self.train['en_pos'])
    val_load = DataLoader(dataset, batch_size = self.batch_size)
    return val_load

#device
#load_data
device = torch.device("cuda")
load_data = DataModule(tokenizer, train_data, validation_data, batch_size = 8)

tokenizer, train_data, validation_data = [], [], []

model.to(device)
epochs = 10

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
      'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
      'weight_decay_rate': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr = 2e-5)
#epochs = 2
#train_loss, valid_loss = [], []

for i in range(epochs):
  train_loss = 0
  model.train()
  for step, batch in enumerate(load_data.train_loader()):
    batch = tuple(t.to(device) for t in batch)
    inp, attn, st, en = batch
    model.zero_grad()
    optimizer.zero_grad()
    outputs = model(inp, attn, start_positions=st, end_positions=en)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
  print('Mean Epoch Loss:', train_loss/len(load_data.train_loader())) 

  model.eval()
  val_loss = 0
  for step, batch in enumerate(load_data.valid_loader()):
    batch = tuple(t.to(device) for t in batch)
    inp, attn, st, en = batch
    model.zero_grad()
    optimizer.zero_grad()
    outputs = model(inp, attn, start_positions=st, end_positions=en)
    loss = outputs.loss

    val_loss += loss.item()
  print('Mean validation Loss:', val_loss/len(load_data.valid_loader()))

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
      'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
      'weight_decay_rate': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr = 2e-5)

print(predict_int)
print(len(load_data.valid_loader())*8)

model.to(device)
model.eval()
em = 0
interval = []
for step, batch in enumerate(load_data.valid_loader()):
  batch = tuple(t.to(device) for t in batch)
  inp, attn, st, en = batch
  model.zero_grad()
  optimizer.zero_grad()
  outputs = model(inp, attn)
  loss = outputs.loss
  start_scores = outputs.start_logits
  end_scores = outputs.end_logits
  st_out = torch.argmax(start_scores, axis = 1)
  en_out = torch.argmax(end_scores, axis = 1)
  for i in range(len(st)):
    if st[i] == st_out[i] and en[i] == en_out[i]:
      em = em + 1
    interval.append((st[i], en[i], st_out[i], en_out[i]))

#exact_match = 76149/87600
print(em/len(load_data.valid_loader())*8)

